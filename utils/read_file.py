# import pickle
import pandas as pd

import re
from underthesea import word_tokenize

TRAIN_FILE = '../data/train.crash'
TEST_FILE = '../data/test.crash'

process_train = '../data/train.csv'
process_test = '../data/test.csv'
ABB_FILE = '../data/abb.txt'

replace_list = {"ğŸ‘¹": "khÃ´ng tá»‘t", "ğŸ‘»": "tá»‘t", "ğŸ’ƒ": "tá»‘t",'ğŸ¤™': ' tá»‘t ', 'ğŸ‘': ' tá»‘t ',
"ğŸ’„": "tá»‘t", "ğŸ’": "tá»‘t", "ğŸ’©": "tá»‘t","ğŸ˜•": "khÃ´ng tá»‘t", "ğŸ˜±": "khÃ´ng tá»‘t", "ğŸ˜¸": "tá»‘t",
"ğŸ˜¾": "khÃ´ng tá»‘t", "ğŸš«": "khÃ´ng tá»‘t",  "ğŸ¤¬": "khÃ´ng tá»‘t","ğŸ§š": "tá»‘t", "ğŸ§¡": "tá»‘t",'ğŸ¶':' tá»‘t ',
'ğŸ‘': ' khÃ´ng tá»‘t ', 'ğŸ˜£': ' khÃ´ng tá»‘t ','âœ¨': ' tá»‘t ', 'â£': ' tá»‘t ','â˜€': ' tá»‘t ', 'â™¥': ' tá»‘t ',
'ğŸ¤©': ' tá»‘t ', 'like': ' tá»‘t ', 'ğŸ’Œ': ' tá»‘t ', 'ğŸ¤£': ' tá»‘t ', 'ğŸ–¤': ' tá»‘t ', 'ğŸ¤¤': ' tá»‘t ',
':(': ' khÃ´ng tá»‘t ', 'ğŸ˜¢': ' khÃ´ng tá»‘t ', 'â¤': ' tá»‘t ', 'ğŸ˜': ' tá»‘t ', 'ğŸ˜˜': ' tá»‘t ',
'ğŸ˜ª': ' khÃ´ng tá»‘t ', 'ğŸ˜Š': ' tá»‘t ', '?': ' ? ', 'ğŸ˜': ' tá»‘t ', 'ğŸ’–': ' tá»‘t ', 'ğŸ˜Ÿ': ' khÃ´ng tá»‘t ',
'ğŸ˜­': ' khÃ´ng tá»‘t ', 'ğŸ’¯': ' tá»‘t ', 'ğŸ’—': ' tá»‘t ', 'â™¡': ' tá»‘t ', 'ğŸ’œ': ' tá»‘t ', 'ğŸ¤—': ' tá»‘t ',
'^^': ' tá»‘t ', 'ğŸ˜¨': ' khÃ´ng tá»‘t ', 'â˜º': ' tá»‘t ', 'ğŸ’‹': ' tá»‘t ', 'ğŸ‘Œ': ' tá»‘t ', 'ğŸ˜–': ' khÃ´ng tá»‘t ',
'ğŸ˜€': ' tá»‘t ', ':((': ' khÃ´ng tá»‘t ', 'ğŸ˜¡': ' khÃ´ng tá»‘t ', 'ğŸ˜ ': ' khÃ´ng tá»‘t ', 'ğŸ˜’': ' khÃ´ng tá»‘t ',
'ğŸ™‚': ' tá»‘t ', 'ğŸ˜': ' khÃ´ng tá»‘t ', 'ğŸ˜': ' tá»‘t ', 'ğŸ˜„': ' tá»‘t ','ğŸ˜™': ' tá»‘t ',
'ğŸ˜¤': ' khÃ´ng tá»‘t ', 'ğŸ˜': ' tá»‘t ', 'ğŸ˜†': ' tá»‘t ', 'ğŸ’š': ' tá»‘t ', 'âœŒ': ' tá»‘t ', 'ğŸ’•': ' tá»‘t ',
'ğŸ˜': ' khÃ´ng tá»‘t ', 'ğŸ˜“': ' khÃ´ng tá»‘t ', 'ï¸ğŸ†—ï¸': ' tá»‘t ', 'ğŸ˜‰': ' tá»‘t ', 'ğŸ˜‚': ' tá»‘t ',
':v': '  tá»‘t ', '=))': '  tá»‘t ', 'ğŸ˜‹': ' tá»‘t ', 'ğŸ’“': ' tá»‘t ', 'ğŸ˜': ' khÃ´ng tá»‘t ', ':3': ' tá»‘t ',
'ğŸ˜«': ' khÃ´ng tá»‘t ', 'ğŸ˜¥': ' khÃ´ng tá»‘t ', 'ğŸ˜ƒ': ' tá»‘t ', 'ğŸ˜¬': ' ğŸ˜¬ ', 'ğŸ˜Œ': ' ğŸ˜Œ ',
'ğŸ’›': ' tá»‘t ', 'ğŸ¤': ' tá»‘t ', 'ğŸˆ': ' tá»‘t ', 'ğŸ˜—': ' tá»‘t ', 'ğŸ¤”': ' khÃ´ng tá»‘t ',
'ğŸ˜‘': ' khÃ´ng tá»‘t ', 'ğŸ”¥': ' khÃ´ng tá»‘t ', 'ğŸ™': ' khÃ´ng tá»‘t ', 'ğŸ†—': ' tá»‘t ', 'ğŸ˜»': ' tá»‘t ',
'ğŸ’™': ' tá»‘t ', 'ğŸ’Ÿ': ' tá»‘t ', 'ğŸ˜š': ' tá»‘t ', 'âŒ': ' khÃ´ng tá»‘t ', 'ğŸ‘': ' tá»‘t ', ';)': ' tá»‘t ',
'<3': ' tá»‘t ', 'ğŸŒ': ' tá»‘t ',  'ğŸŒ·': ' tá»‘t ', 'ğŸŒ¸': ' tá»‘t ', 'ğŸŒº': ' tá»‘t ', 'ğŸŒ¼': ' tá»‘t ',
'ğŸ“': ' tá»‘t ', 'ğŸ…': ' tá»‘t ', 'ğŸ¾': ' tá»‘t ', 'ğŸ‘‰': ' tá»‘t ', 'ğŸ’': ' tá»‘t ', 'ğŸ’': ' tá»‘t ',
'ğŸ’¥': ' tá»‘t ', 'ğŸ’ª': ' tá»‘t ', 'ğŸ’°': ' tá»‘t ',  'ğŸ˜‡': ' tá»‘t ', 'ğŸ˜›': ' tá»‘t ', 'ğŸ˜œ': ' tá»‘t ',
'ğŸ™ƒ': ' tá»‘t ', 'ğŸ¤‘': ' tá»‘t ', 'ğŸ¤ª': ' tá»‘t ','â˜¹': ' khÃ´ng tá»‘t ',  'ğŸ’€': ' khÃ´ng tá»‘t ',
'ğŸ˜”': ' khÃ´ng tá»‘t ', 'ğŸ˜§': ' khÃ´ng tá»‘t ', 'ğŸ˜©': ' khÃ´ng tá»‘t ', 'ğŸ˜°': ' khÃ´ng tá»‘t ',
'ğŸ˜³': ' khÃ´ng tá»‘t ', 'ğŸ˜µ': ' khÃ´ng tá»‘t ', 'ğŸ˜¶': ' khÃ´ng tá»‘t ', 'ğŸ™': ' khÃ´ng tá»‘t ',
'6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ',
'ğŸ‰': 'tá»‘t', '5sao': ' 5star ', 'starstarstarstarstar': ' 5star ',
'1 sao': ' 1star ', '1sao': ' 1star ', '2 sao':' 1star ','2sao':' 1star ',
'2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star '}


def abb_file():
	abb_words = {}
	with open(ABB_FILE, 'r', encoding="utf-8") as data_file:
		file_split = data_file.read().split("\n")
		for line in file_split:
			data_line = line.split(":")
			values = data_line[1].strip()
			keys = data_line[0].split(",")
			for key in keys:
				abb_words[key.strip()] = values
	return abb_words

def test_file(dict_abbs, files, save_path):

	for file_type, file_path in files.items():
		if file_type == "test":
			split_re = re.compile("test_\\d{2,}")
		all_reviews = list()
		with open(file_path, 'r', encoding="utf-8") as data_file:
			data_test = data_file.read()
			labels = split_re.findall(data_test)

			file_split = split_re.split(data_test)
			file_split = list(filter(lambda x: x != '', file_split))
			# print(file_split)
			idx = 0
			for label_data in file_split:
				d = {}
				data = label_data.split("\n")
				data = list(filter(lambda x: x != '', data))
				text = ' '.join(data)
				# if text != "" :
				review = process_data(text, dict_abbs)
				d['text'] = review
				d['label'] = labels[idx]
				all_reviews.append(d)
				idx += 1
			reviews_pd = pd.DataFrame.from_dict(all_reviews, orient='columns')
			reviews_pd.to_csv(save_path[file_type], sep=',', encoding='utf-8',
				header=True, columns=['text', 'label'], index=False)

def load_file(dict_abbs, files, save_path):

	for file_type, file_path in files.items():
		if file_type == "test":
			split_re = re.compile("test_\\d{2,}")
		else:
			split_re = re.compile("train_\\d{2,}")
		all_reviews = list()
		with open(file_path, 'r', encoding="utf-8") as data_file:

			file_split = split_re.split(data_file.read())
			file_split = list(filter(lambda x: x != '', file_split))
			
			for label_data in file_split:
				d = {}
				data = label_data.split("\n")
				data = list(filter(lambda x: x != '', data))
				# print(label_data, data)
				if file_type == "train":
					label = data[-1]
					text = ' '.join(data[:-1])
				else:
					text = ' '.join(data)
				if text != "" :
					review = process_data(text, dict_abbs)
					d['text'] = review
					if file_type == "train":
						d['label'] = label
					all_reviews.append(d)
			reviews_pd = pd.DataFrame.from_dict(all_reviews, orient='columns')
			reviews_pd.to_csv(save_path[file_type], sep=',', encoding='utf-8',
				header=True, columns=['text', 'label'], index=False)

def process_data(original, dict_abbs):

	# replace i-con and star by text
	for k, v in replace_list.items():
		original = original.replace(k, v)
	process = UniStd(original)
	# process = re.findall('[a-zA-ZÃ Ã¡Ã£áº¡áº£Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº¹áº»áº½Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­Ä©á»‰á»‹Ã²Ã³Ãµá»á»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹ÃºÅ©á»¥á»§Æ°á»©á»«á»­á»¯á»±á»³á»µá»·á»¹Ã½Ã€ÃÃƒáº áº¢Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃÄ¨á»ˆá»ŠÃ’Ã“Ã•á»Œá»Ã”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢Ã™ÃšÅ¨á»¤á»¦Æ¯á»¨á»ªá»¬á»®á»°á»²á»´á»¶á»¸Ã\s\d\\.,!?\\-/]+', process)
	process = re.sub('[^a-zA-ZÃ Ã¡Ã£áº¡áº£Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº¹áº»áº½Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­Ä©á»‰á»‹Ã²Ã³Ãµá»á»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹ÃºÅ©á»¥á»§Æ°á»©á»«á»­á»¯á»±á»³á»µá»·á»¹Ã½Ã€ÃÃƒáº áº¢Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃÄ¨á»ˆá»ŠÃ’Ã“Ã•á»Œá»Ã”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢Ã™ÃšÅ¨á»¤á»¦Æ¯á»¨á»ªá»¬á»®á»°á»²á»´á»¶á»¸Ã0-9 ]+', ' ', process)
	# process = re.findall(r'[a-zA-ZÃ Ã¡Ã£áº¡áº£Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã¨Ã©áº¹áº»áº½Ãªá»áº¿á»ƒá»…á»‡Ä‘Ã¬Ã­Ä©á»‰á»‹Ã²Ã³Ãµá»á»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã¹ÃºÅ©á»¥á»§Æ°á»©á»«á»­á»¯á»±á»³á»µá»·á»¹Ã½Ã€ÃÃƒáº áº¢Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»‚á»„á»†ÄÃŒÃÄ¨á»ˆá»ŠÃ’Ã“Ã•á»Œá»Ã”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢Ã™ÃšÅ¨á»¤á»¦Æ¯á»¨á»ªá»¬á»®á»°á»²á»´á»¶á»¸Ã\s\d]+', process)

	text = process.lower()
	for key, value in dict_abbs.items():
		# find word with spaces in text
		# if (' ' + key + ' ') in (' ' + text + ' '):
		# 	text = text.replace(' ' + key + ' ', ' ' + value + ' ')
		match_string = r'\b' + key + r'\b'
		regex = re.compile(match_string, re.S)
		text = regex.sub(lambda m: m.group().replace(key,value,1), text)
	text = text.replace('\xa0', '') # error in encode
	text = re.sub('[",]', '', text)
	text = re.sub(r'([a-z])\1+', r'',text) # remove duplicate charater in word: Æ¡iiiiiiiiiii
	text = ' '.join(text.split())
	text = text.strip()
	text = word_tokenize(text, format="text")

	return text

def UniStd_L(str):
	return str.\
		replace(u'aÌ€',u'Ã ').replace(u'aÌƒ',u'Ã£').replace(u'aÌ‰',u'áº£').replace(u'aÌ',u'Ã¡').replace(u'aÌ£',u'áº¡').\
		replace(u'ÄƒÌ€',u'áº±').replace(u'ÄƒÌƒ',u'áºµ').replace(u'ÄƒÌ‰',u'áº³').replace(u'ÄƒÌ',u'áº¯').replace(u'ÄƒÌ£',u'áº·').\
		replace(u'Ã¢Ì€',u'áº§').replace(u'Ã¢Ìƒ',u'áº«').replace(u'Ã¢Ì‰',u'áº©').replace(u'Ã¢Ì',u'áº¥').replace(u'Ã¢Ì£',u'áº­').\
		replace(u'yÌ€',u'á»³').replace(u'yÌƒ',u'á»¹').replace(u'yÌ‰',u'á»·').replace(u'yÌ',u'Ã½').replace(u'yÌ£',u'á»µ').\
		replace(u'iÌ€',u'Ã¬').replace(u'iÌƒ',u'Ä©').replace(u'iÌ‰',u'á»‰').replace(u'iÌ',u'Ã­').replace(u'iÌ£',u'á»‹').\
		replace(u'uÌ€',u'Ã¹').replace(u'uÌƒ',u'Å©').replace(u'uÌ‰',u'á»§').replace(u'uÌ',u'Ãº').replace(u'uÌ£',u'á»¥').\
		replace(u'Æ°Ì€',u'á»«').replace(u'Æ°Ìƒ',u'á»¯').replace(u'Æ°Ì‰',u'á»­').replace(u'Æ°Ì',u'á»©').replace(u'Æ°Ì£',u'á»±').\
		replace(u'eÌ€',u'Ã¨').replace(u'eÌƒ',u'áº½').replace(u'eÌ‰',u'áº»').replace(u'eÌ',u'Ã©').replace(u'eÌ£',u'áº¹').\
		replace(u'ÃªÌ€',u'á»').replace(u'ÃªÌƒ',u'á»…').replace(u'ÃªÌ‰',u'á»ƒ').replace(u'ÃªÌ',u'áº¿').replace(u'ÃªÌ£',u'á»‡').\
		replace(u'oÌ€',u'Ã²').replace(u'oÌƒ',u'Ãµ').replace(u'oÌ‰',u'á»').replace(u'oÌ',u'Ã³').replace(u'oÌ£',u'á»').\
		replace(u'Æ¡Ì€',u'á»').replace(u'Æ¡Ìƒ',u'á»¡').replace(u'Æ¡Ì‰',u'á»Ÿ').replace(u'Æ¡Ì',u'á»›').replace(u'Æ¡Ì£',u'á»£').\
		replace(u'Ã´Ì€',u'á»“').replace(u'Ã´Ìƒ',u'á»—').replace(u'Ã´Ì‰',u'á»•').replace(u'Ã´Ì',u'á»‘').replace(u'Ã´Ì£',u'á»™').\
		replace(u'Ã²a',u'oÃ ').replace(u'Ãµa',u'oÃ£').replace(u'á»a',u'oáº£').replace(u'Ã³a',u'oÃ¡').replace(u'á»a',u'oáº¡').\
		replace(u'Ã²e',u'oÃ¨').replace(u'Ãµe',u'oáº½').replace(u'á»e',u'oáº»').replace(u'Ã³e',u'oÃ©').replace(u'á»e',u'oáº¹').\
		replace(u'Ã¹y',u'uá»³').replace(u'Å©y',u'uá»¹').replace(u'á»§y',u'uá»·').replace(u'Ãºy',u'uÃ½').replace(u'á»¥y',u'uá»µ').\
		replace(u'aÃ³',u'Ã¡o')

def UniStd_H(str):
	return str.\
		replace(u'AÌ€',u'Ã€').replace(u'AÌƒ',u'Ãƒ').replace(u'AÌ‰',u'áº¢').replace(u'AÌ',u'Ã').replace(u'AÌ£',u'áº ').\
		replace(u'Ä‚Ì€',u'áº°').replace(u'Ä‚Ìƒ',u'áº´').replace(u'Ä‚Ì‰',u'áº²').replace(u'Ä‚Ì',u'áº®').replace(u'Ä‚Ì£',u'áº¶').\
		replace(u'Ã‚Ì€',u'áº¦').replace(u'Ã‚Ìƒ',u'áºª').replace(u'Ã‚Ì‰',u'áº¨').replace(u'Ã‚Ì',u'áº¤').replace(u'Ã‚Ì£',u'áº¬').\
		replace(u'YÌ€',u'á»²').replace(u'YÌƒ',u'á»¸').replace(u'YÌ‰',u'á»¶').replace(u'YÌ',u'Ã').replace(u'YÌ£',u'á»´').\
		replace(u'IÌ€',u'ÃŒ').replace(u'IÌƒ',u'Ä¨').replace(u'IÌ‰',u'á»ˆ').replace(u'IÌ',u'Ã').replace(u'IÌ£',u'á»Š').\
		replace(u'UÌ€',u'Ã™').replace(u'UÌƒ',u'Å¨').replace(u'UÌ‰',u'á»¦').replace(u'UÌ',u'Ãš').replace(u'UÌ£',u'á»¤').\
		replace(u'Æ¯Ì€',u'á»ª').replace(u'Æ¯Ìƒ',u'á»®').replace(u'Æ¯Ì‰',u'á»¬').replace(u'Æ¯Ì',u'á»¨').replace(u'Æ¯Ì£',u'á»°').\
		replace(u'EÌ€',u'Ãˆ').replace(u'EÌƒ',u'áº¼').replace(u'EÌ‰',u'áºº').replace(u'EÌ',u'Ã‰').replace(u'EÌ£',u'áº¸').\
		replace(u'ÃŠÌ€',u'á»€').replace(u'ÃŠÌƒ',u'á»„').replace(u'ÃŠÌ‰',u'á»‚').replace(u'ÃŠÌ',u'áº¾').replace(u'ÃŠÌ£',u'á»†').\
		replace(u'OÌ€',u'Ã’').replace(u'OÌƒ',u'Ã•').replace(u'OÌ‰',u'á»').replace(u'OÌ',u'Ã“').replace(u'OÌ£',u'á»Œ').\
		replace(u'Æ Ì€',u'á»œ').replace(u'Æ Ìƒ',u'á» ').replace(u'Æ Ì‰',u'á»').replace(u'Æ Ì',u'á»š').replace(u'Æ Ì£',u'á»¢').\
		replace(u'Ã”Ì€',u'á»’').replace(u'Ã”Ìƒ',u'á»–').replace(u'Ã”Ì‰',u'á»”').replace(u'Ã”Ì',u'á»').replace(u'Ã”Ì£',u'á»˜').\
		replace(u'Ã’A',u'OÃ€').replace(u'Ã•A',u'OÃƒ').replace(u'á»A',u'Oáº¢').replace(u'Ã“A',u'OÃ').replace(u'á»ŒA',u'Oáº ').\
		replace(u'Ã’E',u'OÃˆ').replace(u'Ã•E',u'Oáº¼').replace(u'á»E',u'Oáºº').replace(u'Ã“E',u'OÃ‰').replace(u'á»ŒE',u'Oáº¸').\
		replace(u'Ã™Y',u'Uá»²').replace(u'Å¨Y',u'Uá»¸').replace(u'á»¦Y',u'Uá»¶').replace(u'ÃšY',u'UÃ').replace(u'á»¤Y',u'Uá»´')

def UniStd(str):
	return UniStd_L(UniStd_H(str)).\
		replace(u'Ã²A',u'oÃ ').replace(u'ÃµA',u'oÃ£').replace(u'á»A',u'oáº£').replace(u'Ã³A',u'oÃ¡').replace(u'á»A',u'oáº¡').\
		replace(u'Ã²E',u'oÃ¨').replace(u'ÃµE',u'oáº½').replace(u'á»E',u'oáº»').replace(u'Ã³E',u'oÃ©').replace(u'á»E',u'oáº¹').\
		replace(u'Ã¹Y',u'uá»³').replace(u'Å©Y',u'uá»¹').replace(u'á»§Y',u'uá»·').replace(u'ÃºY',u'uÃ½').replace(u'á»¥Y',u'uá»µ').\
		replace(u'Ã’a',u'OÃ ').replace(u'Ã•a',u'OÃ£').replace(u'á»a',u'Oáº£').replace(u'Ã“a',u'OÃ¡').replace(u'á»Œa',u'Oáº¡').\
		replace(u'Ã’e',u'OÃ¨').replace(u'Ã•e',u'Oáº½').replace(u'á»e',u'Oáº»').replace(u'Ã“e',u'OÃ©').replace(u'á»Œe',u'Oáº¹').\
		replace(u'Ã™y',u'Uá»³').replace(u'Å¨y',u'Uá»¹').replace(u'á»¦y',u'Uá»·').replace(u'Ãšy',u'UÃ½').replace(u'á»¤y',u'Uá»µ')

# print(load_file())
dict_abbs = abb_file()
# load_file(dict_abbs, TRAIN_FILE, process_train)
files = {"test": TEST_FILE}
save_path = {"test": process_test}
test_file(dict_abbs, files, save_path)
# print(abb_words)