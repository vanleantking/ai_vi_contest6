{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[w2vector].homework8.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"0QKMHQzkd4DH","colab_type":"code","outputId":"ce8ea287-0512-43d2-9e31-7e21acedece7","executionInfo":{"status":"ok","timestamp":1564286547059,"user_tz":-420,"elapsed":49913,"user":{"displayName":"van le","photoUrl":"https://lh6.googleusercontent.com/-try3WApC_JQ/AAAAAAAAAAI/AAAAAAAACCA/cYudomZr1hM/s64/photo.jpg","userId":"12393812284708270759"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DYp26XNCyNKY","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import time\n","import random\n","import argparse\n","import numpy as np\n","\n","import torch\n","from sklearn import metrics\n","import torch.optim as optim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mkd-KMheix4D","colab_type":"code","outputId":"88a2eb65-6eea-4011-b1b5-848faa5ba374","executionInfo":{"status":"ok","timestamp":1564286558258,"user_tz":-420,"elapsed":1002,"user":{"displayName":"van le","photoUrl":"https://lh6.googleusercontent.com/-try3WApC_JQ/AAAAAAAAAAI/AAAAAAAACCA/cYudomZr1hM/s64/photo.jpg","userId":"12393812284708270759"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["path = '/content/drive/My Drive/NLP_COLAB/homework8/'\n","sys.path.append(path)\n","print(sys.path)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['', '/env/python', '/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '/usr/local/lib/python3.6/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.6/dist-packages/IPython/extensions', '/root/.ipython', '/content/drive/My Drive/NLP_COLAB/homework8/']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ByNieo0CkGGj","colab_type":"code","colab":{}},"source":["from utils.core_nns import BiRNN as fNN\n","from utils.other_utils import Progbar, Timer, SaveloadHP\n","from utils.data_utils import Vocab, Data2tensor, Txtfile, seqPAD, Embeddings"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0AeJ8p9_evV5","colab_type":"code","colab":{}},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Mar 28 03:38:32 2018\n","\n","@author: duytinvo\n","\"\"\"\n","\n","\n","seed_num = 12345\n","random.seed(seed_num)\n","torch.manual_seed(seed_num)\n","np.random.seed(seed_num)\n","\n","\n","class Classifier(object):\n","    def __init__(self, args=None):\n","        \n","        self.args = args  \n","        self.device = torch.device(\"cuda:0\" if self.args.use_cuda else \"cpu\")\n","        # word_layers = 1\n","        word_bidirect = True        \n","        word_HPs = [self.args.word_nnmode, len(self.args.vocab.w2i), self.args.word_dim,\n","                    self.args.word_pred_embs, self.args.word_hidden_dim, self.args.dropout,\n","                    self.args.word_layers, word_bidirect, self.args.zero_padding, self.args.word_att]\n","        \n","        self.model = fNN(word_HPs=word_HPs, filter_size = self.args.filter_size, out_channels = self.args.out_channels,\n","        use_batchnorm = self.args.use_batchnorm, num_labels=len(self.args.vocab.l2i)).to(self.device)\n","\n","        if args.optimizer.lower() == \"adamax\":\n","            self.optimizer = optim.Adamax(self.model.parameters(), lr=self.args.lr)\n","        elif args.optimizer.lower() == \"adam\":\n","            self.optimizer = optim.Adam(self.model.parameters(), lr=self.args.lr)\n","        elif args.optimizer.lower() == \"adadelta\":\n","            self.optimizer = optim.Adadelta(self.model.parameters(), lr=self.args.lr)\n","        elif args.optimizer.lower() == \"adagrad\":\n","            self.optimizer = optim.Adagrad(self.model.parameters(), lr=self.args.lr)\n","        else:\n","            self.optimizer = optim.SGD(self.model.parameters(), lr=self.args.lr, momentum=0.9)\n","        \n","        self.word2idx = self.args.vocab.wd2idx(vocab_words=self.args.vocab.w2i, allow_unk=True, start_end=self.args.start_end)\n","        self.tag2idx = self.args.vocab.tag2idx(vocab_tags=self.args.vocab.l2i)\n","\n","    def evaluate_batch(self, eva_data):\n","        with torch.no_grad():\n","            wl = self.args.vocab.wl\n","            batch_size = self.args.batch_size  \n","             ## set model in eval model\n","            self.model.eval()\n","            start = time.time()\n","            y_true = Data2tensor.idx2tensor([], self.device)\n","            y_pred = Data2tensor.idx2tensor([], self.device)\n","            for i,(words, label_ids) in enumerate(self.args.vocab.minibatches(eva_data, batch_size=batch_size)):\n","                word_ids, sequence_lengths = seqPAD.pad_sequences(words, pad_tok=0, wthres=wl)\n","        \n","                data_tensors = Data2tensor.sort_tensors(label_ids, word_ids,sequence_lengths, self.device)\n","                label_tensor, word_tensor, sequence_lengths, word_seq_recover = data_tensors\n","\n","                y_true = torch.cat([y_true,label_tensor])\n","                label_score = self.model(word_tensor, sequence_lengths)\n","                label_prob, label_pred = self.model.inference(label_score, k=1)\n","                \n","                y_pred = torch.cat([y_pred, label_pred])\n","            measures = Classifier.class_metrics(y_true, y_pred.squeeze())\n","            #measures = Classifier.class_metrics(y_true.data.cpu().numpy(), y_pred.squeeze().data.cpu().numpy())\n","\n","            end = time.time() - start\n","            speed = len(y_true)/end\n","        return measures, speed\n","\n","    def train_batch(self,train_data):\n","        wl = self.args.vocab.wl\n","        clip_rate = self.args.clip\n","        \n","        batch_size = self.args.batch_size\n","        num_train = len(train_data)\n","        total_batch = num_train//batch_size+1\n","        prog = Progbar(target=total_batch)\n","        ## set model in train model\n","        self.model.train()\n","        train_loss = []\n","        for i,(words, label_ids) in enumerate(self.args.vocab.minibatches(train_data, batch_size=batch_size)):\n","            word_ids, sequence_lengths = seqPAD.pad_sequences(words, pad_tok=0, wthres=wl)\n","\n","            data_tensors = Data2tensor.sort_tensors(label_ids, word_ids,sequence_lengths,self.device)\n","            label_tensor, word_tensor, sequence_lengths, word_seq_recover = data_tensors\n","\n","            self.model.zero_grad()\n","            label_score = self.model(word_tensor, sequence_lengths)\n","            batch_loss = self.model.NLL_loss(label_score, label_tensor)\n","            train_loss.append(batch_loss.item())\n","            \n","            batch_loss.backward()\n","            \n","            if clip_rate>0:\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_rate)\n","                \n","            self.optimizer.step()\n","            \n","            prog.update(i + 1, [(\"Train loss\", batch_loss.item())])\n","        return np.mean(train_loss)\n","\n","    def lr_decay(self, epoch):\n","        lr = self.args.lr/(1+self.args.decay_rate*epoch)\n","        print(\"INFO: - Learning rate is setted as: %f\"%lr)\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","    def train(self):            \n","        train_data = Txtfile(self.args.train_file, firstline=False, word2idx=self.word2idx, tag2idx=self.tag2idx)\n","        dev_data = Txtfile(self.args.dev_file, firstline=False, word2idx=self.word2idx, tag2idx=self.tag2idx)\n","        test_data = Txtfile(self.args.test_file, firstline=False, word2idx=self.word2idx, tag2idx=self.tag2idx)\n","\n","        max_epochs = self.args.max_epochs\n","        saved_epoch = 0\n","        best_dev = -1\n","        best_metrics = {}\n","\n","        nepoch_no_imprv = 0\n","        epoch_start = time.time()\n","        for epoch in range(max_epochs):\n","            if self.args.decay_rate>0: \n","                self.lr_decay(epoch)\n","            print(\"Epoch: %s/%s\" %(epoch,max_epochs))\n","            train_loss = self.train_batch(train_data)\n","            # evaluate on developing data\n","            dev_metrics, dev_speed = self.evaluate_batch(dev_data)\n","            dev_metric_standard = dev_metrics[\"prf_macro\"][2]\n","            if dev_metric_standard > best_dev:\n","                nepoch_no_imprv = 0\n","                saved_epoch = epoch\n","                best_dev = dev_metric_standard\n","                best_metrics = dev_metrics\n","                print(\"UPDATES: - New improvement\")  \n","                print(\"         - Train loss: %.4f\"%train_loss)\n","                print(\"         - Dev acc: %.2f(%%); Dev P: %.2f(%%); Dev R: %.2f(%%);Dev F1: %.2f(%%); Dev speed: %.2f(sent/s)\"%(100*dev_metrics[\"acc\"],\n","                      100*dev_metrics[\"prf_macro\"][0], 100*dev_metrics[\"prf_macro\"][1], 100*dev_metrics[\"prf_macro\"][2], dev_speed))\n","                print(\"         - Save the model to %s at epoch %d\"%(self.args.model_name,saved_epoch))\n","                # Conver model to CPU to avoid out of GPU memory\n","                self.model.to(\"cpu\")\n","                torch.save(self.model.state_dict(), self.args.model_name)\n","                self.model.to(self.device)\n","            else:\n","                nepoch_no_imprv += 1\n","                if nepoch_no_imprv >= self.args.patience:\n","                    self.model.load_state_dict(torch.load(self.args.model_name))\n","                    self.model.to(self.device)\n","                    test_metrics, test_speed = self.evaluate_batch(test_data)\n","                    print(\"\\nSUMMARY: - Early stopping after %d epochs without improvements\"%(nepoch_no_imprv))\n","                    print(\"         - Dev acc: %.2f(%%); Dev P: %.2f(%%); Dev R: %.2f(%%);Dev F1: %.2f(%%)\"%(100*best_metrics[\"acc\"],\n","                          100*best_metrics[\"prf_macro\"][0], 100*best_metrics[\"prf_macro\"][1], 100*best_metrics[\"prf_macro\"][2]))\n","                    print(\"         - Load the best model from: %s at epoch %d\"%(self.args.model_name,saved_epoch))                    \n","                    print(\"         - Test acc: %.2f(%%); Test P: %.2f(%%); Test R: %.2f(%%);Test F1: %.2f(%%); \"\n","                          \"Test speed: %.2f(sent/s)\"%(100*test_metrics[\"acc\"], 100*test_metrics[\"prf_macro\"][0],\n","                                                      100*test_metrics[\"prf_macro\"][1],\n","                                                      100*test_metrics[\"prf_macro\"][2], test_speed))\n","                \n","                    return\n","\n","            epoch_finish = Timer.timeEst(epoch_start,(epoch+1)/max_epochs)\n","            print(\"\\nINFO: - Trained time(Remained time for %d epochs: %s\"%(max_epochs, epoch_finish))\n","        \n","        self.model.load_state_dict(torch.load(self.args.model_name))\n","        self.model.to(self.device)\n","        test_metrics, test_speed = self.evaluate_batch(test_data)\n","        print(\"\\nSUMMARY: - Completed %d epoches\"%(max_epochs))\n","        print(\"         - Dev acc: %.2f(%%); Dev P: %.2f(%%); Dev R: %.2f(%%);Dev F1: %.2f(%%)\"%(100*best_metrics[\"acc\"],\n","              100*best_metrics[\"prf_macro\"][0], 100*best_metrics[\"prf_macro\"][1], 100*best_metrics[\"prf_macro\"][2]))\n","        print(\"         - Load the best model from: %s at epoch %d\"%(self.args.model_name,saved_epoch))\n","        print(\"         - Test acc: %.2f(%%); Test P: %.2f (%%); Test R: %.2f(%%);Test F1: %.2f(%%); Test speed: %.2f(sent/s)\"%(100*test_metrics[\"acc\"],\n","              100*test_metrics[\"prf_macro\"][0], 100*test_metrics[\"prf_macro\"][1], 100*test_metrics[\"prf_macro\"][2], test_speed))\n","        return \n","\n","    def predict(self, sent, k=1):\n","        \"\"\"\n","\n","        :param sent: processed sentence\n","        :param asp: an aspect mentioned inside sent\n","        :param k: int\n","        :return: top k predictions\n","        \"\"\"\n","        wl = self.args.vocab.wl\n","         ## set model in eval model\n","        self.model.eval()\n","        \n","        fake_label = [0]        \n","        words = self.word2idx(sent)\n","        word_ids, sequence_lengths = seqPAD.pad_sequences([words], pad_tok=0, wthres=wl)\n","    \n","        data_tensors = Data2tensor.sort_tensors(fake_label, word_ids, sequence_lengths, self.device)\n","        fake_label_tensor, word_tensor, sequence_lengths, word_seq_recover = data_tensors\n","\n","        label_score = self.model(word_tensor, sequence_lengths)\n","        label_prob, label_pred = self.model.inference(label_score, k)\n","        return label_prob, label_pred \n","    \n","    @staticmethod\n","    def class_metrics(y_true, y_pred):\n","        acc = metrics.accuracy_score(y_true, y_pred)  \n","        f1_ma = metrics.precision_recall_fscore_support(y_true, y_pred, average='macro')    \n","        f1_we = metrics.precision_recall_fscore_support(y_true, y_pred, average='weighted') \n","        f1_no = metrics.precision_recall_fscore_support(y_true, y_pred, average=None)  \n","        measures = {\"acc\":acc, \"prf_macro\":f1_ma, \"prf_weighted\":f1_we, \"prf_individual\":f1_no}\n","        return measures\n","\n","\n","def build_data(args):    \n","    print(\"Building dataset...\")\n","    model_dir, _ = os.path.split(args.model_args)\n","    if not os.path.exists(model_dir): \n","        os.mkdir(model_dir)\n","\n","    vocab = Vocab(wl_th=args.word_thres, cutoff=args.cutoff)\n","    vocab.build([args.train_file, args.dev_file, args.test_file], firstline=False)\n","    args.vocab = vocab\n","    if args.emb_file != \"\":\n","        args.word_pred_embs = Embeddings.get_W(args.emb_file,wsize=args.word_dim,vocabx=vocab.w2i)\n","    else:\n","        args.word_pred_embs = None\n","    SaveloadHP.save(args, args.model_args)\n","    return args\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"khnAZSmAoqpn","colab_type":"code","outputId":"518f474e-8882-4028-fe40-56ae6554fba5","executionInfo":{"status":"ok","timestamp":1564299215069,"user_tz":-420,"elapsed":12555915,"user":{"displayName":"van le","photoUrl":"https://lh6.googleusercontent.com/-try3WApC_JQ/AAAAAAAAAAI/AAAAAAAACCA/cYudomZr1hM/s64/photo.jpg","userId":"12393812284708270759"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if __name__ == '__main__':\n","      args= SaveloadHP.load('/content/drive/My Drive/NLP_COLAB/homework8/data/classifier.args')\n","      args.train_file =\"/content/drive/My Drive/NLP_COLAB/homework8/data/train.txt\"\n","\n","      args.dev_file = \"/content/drive/My Drive/NLP_COLAB/homework8/data/val.txt\"\n","\n","      args.test_file =\"/content/drive/My Drive/NLP_COLAB/homework8/data/test.txt\"\n","      args.emb_file =\"/content/drive/My Drive/NLP_COLAB/homework8/data/glove.6B.50d.txt\"\n","      args.model_name =\"/content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m\"\n","\n","      args.model_args =\"/content/drive/My Drive/NLP_COLAB/homework8/data/classifier.args\"\n","      args.word_dim = 50\n","      args.word_hidden_dim = 100\n","      args.use_batchnorm = True\n","      args.dropout = 0.4\n","      args.use_cuda = False\n","      args.word_att = True # enable attention for train lstm\n","      args.filter_size = [2,3,4,5] # fitler size for cnn network\n","      args.out_channels = 32 # Choose out_channel for cnn network\n","    \n","      args = build_data(args)\n","\n","      classifier = Classifier(args)\n","\n","      classifier.train()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Reading hyper-parameters from /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.args\n","Building dataset...\n","Extracting vocabulary:\n","\t24988 total samples, 2652560 total tokens, 24988 total labels\n","\t47091 unique tokens, 5 unique labels\n","\t21802 unique tokens appearing at least 2 times\n","Extracting pretrained embeddings:\n","\t400000 pre-trained word embeddings\n","Mapping to vocabulary:\n","\t3019 randomly word vectors;\n","\t0 partially word vectors;\n","\t18787 pre-trained embeddings.\n","Writing hyper-parameters into /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.args\n","INFO: - Learning rate is setted as: 0.001000\n","Epoch: 0/32\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n","  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["625/625 [==============================] - 539s - Train loss: 1.3554   \n","UPDATES: - New improvement\n","         - Train loss: 1.3554\n","         - Dev acc: 49.98(%); Dev P: 50.25(%); Dev R: 49.85(%);Dev F1: 49.06(%); Dev speed: 294.69(sent/s)\n","         - Save the model to /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 0\n","\n","INFO: - Trained time(Remained time for 32 epochs: 9m 9s (- 284m 5s)\n","INFO: - Learning rate is setted as: 0.000952\n","Epoch: 1/32\n","625/625 [==============================] - 541s - Train loss: 1.1269   \n","UPDATES: - New improvement\n","         - Train loss: 1.1269\n","         - Dev acc: 53.14(%); Dev P: 52.57(%); Dev R: 53.04(%);Dev F1: 51.89(%); Dev speed: 298.36(sent/s)\n","         - Save the model to /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 1\n","\n","INFO: - Trained time(Remained time for 32 epochs: 18m 19s (- 274m 52s)\n","INFO: - Learning rate is setted as: 0.000909\n","Epoch: 2/32\n","625/625 [==============================] - 546s - Train loss: 1.0490   \n","UPDATES: - New improvement\n","         - Train loss: 1.0490\n","         - Dev acc: 56.30(%); Dev P: 55.24(%); Dev R: 56.15(%);Dev F1: 54.96(%); Dev speed: 298.22(sent/s)\n","         - Save the model to /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 2\n","\n","INFO: - Trained time(Remained time for 32 epochs: 27m 34s (- 266m 35s)\n","INFO: - Learning rate is setted as: 0.000870\n","Epoch: 3/32\n","625/625 [==============================] - 551s - Train loss: 0.9977   \n","UPDATES: - New improvement\n","         - Train loss: 0.9977\n","         - Dev acc: 57.14(%); Dev P: 56.38(%); Dev R: 56.96(%);Dev F1: 56.02(%); Dev speed: 289.02(sent/s)\n","         - Save the model to /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 3\n","\n","INFO: - Trained time(Remained time for 32 epochs: 36m 55s (- 258m 27s)\n","INFO: - Learning rate is setted as: 0.000833\n","Epoch: 4/32\n","625/625 [==============================] - 539s - Train loss: 0.9620   \n","UPDATES: - New improvement\n","         - Train loss: 0.9620\n","         - Dev acc: 57.50(%); Dev P: 57.66(%); Dev R: 57.31(%);Dev F1: 57.10(%); Dev speed: 295.42(sent/s)\n","         - Save the model to /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 4\n","\n","INFO: - Trained time(Remained time for 32 epochs: 46m 3s (- 248m 42s)\n","INFO: - Learning rate is setted as: 0.000800\n","Epoch: 5/32\n","625/625 [==============================] - 539s - Train loss: 0.9264   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 55m 11s (- 239m 9s)\n","INFO: - Learning rate is setted as: 0.000769\n","Epoch: 6/32\n","625/625 [==============================] - 536s - Train loss: 0.9038   \n","UPDATES: - New improvement\n","         - Train loss: 0.9038\n","         - Dev acc: 58.46(%); Dev P: 58.60(%); Dev R: 58.28(%);Dev F1: 58.17(%); Dev speed: 299.98(sent/s)\n","         - Save the model to /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 6\n","\n","INFO: - Trained time(Remained time for 32 epochs: 64m 16s (- 229m 32s)\n","INFO: - Learning rate is setted as: 0.000741\n","Epoch: 7/32\n","625/625 [==============================] - 530s - Train loss: 0.8703   \n","UPDATES: - New improvement\n","         - Train loss: 0.8703\n","         - Dev acc: 58.62(%); Dev P: 58.62(%); Dev R: 58.49(%);Dev F1: 58.41(%); Dev speed: 300.35(sent/s)\n","         - Save the model to /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 7\n","\n","INFO: - Trained time(Remained time for 32 epochs: 73m 15s (- 219m 46s)\n","INFO: - Learning rate is setted as: 0.000714\n","Epoch: 8/32\n","625/625 [==============================] - 530s - Train loss: 0.8460   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 82m 13s (- 210m 8s)\n","INFO: - Learning rate is setted as: 0.000690\n","Epoch: 9/32\n","625/625 [==============================] - 541s - Train loss: 0.8177   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 91m 23s (- 201m 4s)\n","INFO: - Learning rate is setted as: 0.000667\n","Epoch: 10/32\n","625/625 [==============================] - 539s - Train loss: 0.8021   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 100m 31s (- 191m 54s)\n","INFO: - Learning rate is setted as: 0.000645\n","Epoch: 11/32\n","625/625 [==============================] - 533s - Train loss: 0.7828   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 109m 33s (- 182m 35s)\n","INFO: - Learning rate is setted as: 0.000625\n","Epoch: 12/32\n","625/625 [==============================] - 543s - Train loss: 0.7626   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 118m 45s (- 173m 34s)\n","INFO: - Learning rate is setted as: 0.000606\n","Epoch: 13/32\n","625/625 [==============================] - 544s - Train loss: 0.7537   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 127m 58s (- 164m 32s)\n","INFO: - Learning rate is setted as: 0.000588\n","Epoch: 14/32\n","625/625 [==============================] - 535s - Train loss: 0.7293   \n","UPDATES: - New improvement\n","         - Train loss: 0.7293\n","         - Dev acc: 59.38(%); Dev P: 59.19(%); Dev R: 59.26(%);Dev F1: 59.20(%); Dev speed: 301.37(sent/s)\n","         - Save the model to /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 14\n","\n","INFO: - Trained time(Remained time for 32 epochs: 137m 3s (- 155m 19s)\n","INFO: - Learning rate is setted as: 0.000571\n","Epoch: 15/32\n","625/625 [==============================] - 547s - Train loss: 0.7227   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 146m 27s (- 146m 27s)\n","INFO: - Learning rate is setted as: 0.000556\n","Epoch: 16/32\n","625/625 [==============================] - 519s - Train loss: 0.7017   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 155m 15s (- 136m 59s)\n","INFO: - Learning rate is setted as: 0.000541\n","Epoch: 17/32\n","625/625 [==============================] - 527s - Train loss: 0.6870   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 164m 11s (- 127m 41s)\n","INFO: - Learning rate is setted as: 0.000526\n","Epoch: 18/32\n","625/625 [==============================] - 526s - Train loss: 0.6793   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 173m 5s (- 118m 26s)\n","INFO: - Learning rate is setted as: 0.000513\n","Epoch: 19/32\n","625/625 [==============================] - 527s - Train loss: 0.6574   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 182m 1s (- 109m 12s)\n","INFO: - Learning rate is setted as: 0.000500\n","Epoch: 20/32\n","625/625 [==============================] - 531s - Train loss: 0.6463   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 191m 1s (- 100m 3s)\n","INFO: - Learning rate is setted as: 0.000488\n","Epoch: 21/32\n","625/625 [==============================] - 531s - Train loss: 0.6407   \n","\n","INFO: - Trained time(Remained time for 32 epochs: 200m 1s (- 90m 55s)\n","INFO: - Learning rate is setted as: 0.000476\n","Epoch: 22/32\n","625/625 [==============================] - 522s - Train loss: 0.6237   \n","\n","SUMMARY: - Early stopping after 8 epochs without improvements\n","         - Dev acc: 59.38(%); Dev P: 59.19(%); Dev R: 59.26(%);Dev F1: 59.20(%)\n","         - Load the best model from: /content/drive/My Drive/NLP_COLAB/homework8/data/classifier.m at epoch 14\n","         - Test acc: 59.86(%); Test P: 59.56(%); Test R: 59.86(%);Test F1: 59.66(%); Test speed: 292.06(sent/s)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wEMCUSZdystq","colab_type":"code","colab":{}},"source":["pip freeze"],"execution_count":0,"outputs":[]}]}